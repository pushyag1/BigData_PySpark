{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pyspark-All codes_Compiled.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushyag1/BigData_PySpark/blob/main/Pyspark_UpdatedCompiled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "PiobpRXZQOOz",
        "outputId": "ef9ab50e-9987-4444-c752-a9bf1f5cca95"
      },
      "source": [
        "#@title PySpark Setup(run me!)\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-3.0.2/spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.2-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.2-bin-hadoop2.7\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local\")\\\n",
        "        .appName(\"Colab\")\\\n",
        "        .config('spark.ui.port', '4050')\\\n",
        "        .getOrCreate()\n",
        "\n",
        "spark\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r                                                                               \rHit:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Connecting to security.u\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r                                                                               \rGet:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [Waiting for headers] [6 In\r                                                                               \rGet:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,759 kB]\n",
            "Fetched 2,027 kB in 3s (756 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://f5b2f50aab0b:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Colab</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fbf6e5b1250>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpfxRv899Zag"
      },
      "source": [
        "#@title Dynamic Input(select your choices and run the cell)\n",
        "Date_to_calculate_age = '2021-02-01' #@param {type:\"date\"}\n",
        "Choose_a_model_version = 'ESRD-P1' #@param [\"V22\",\"V24\", \"RX\", \"ESRD-P1\",\"ESRD-P2\"]\n",
        "Choose_a_model_year = '2021' #@param [\"2020\",\"2021\"]\n",
        "Sex_Age_edits_required = 'Yes' #@param [\"Yes\",\"No\"]"
      ],
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2FnTwwJwZWi",
        "outputId": "8c7df78d-c3ca-4a4f-a4a9-818f91052ebd"
      },
      "source": [
        "#@title Age Calculation\n",
        "import pandas \n",
        "import os\n",
        "import pyspark.sql.functions as func\n",
        "from pyspark.sql.functions import datediff, to_date, lit\n",
        "from pyspark.sql.functions import struct\n",
        "from pyspark.sql.types import IntegerType\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "\n",
        "#Loading Persons file\n",
        "df1 = pandas.read_excel (\"/content/Person-file.xlsx\")\n",
        "#df1=df.drop(['Unnamed: 8', 'Unnamed: 9','Unnamed: 10'], axis=1)\n",
        "df1 = spark.createDataFrame(df1)\n",
        "\n",
        "df2=df1.withColumn(\"Age\",(datediff(to_date(lit(Date_to_calculate_age)),to_date(\"DOB\",\"yyyy/MM/dd\")))/366)\n",
        "df3 = df2.withColumn(\"Age\", func.round(df2[\"Age\"], 2).cast('integer'))\n",
        "\n",
        "#Condition for AGE and OREC\n",
        "def func(Age, OREC):\n",
        "  if Age == 64 and OREC == 0:\n",
        "    return 65\n",
        "  elif Age < 0 :\n",
        "    return 0\n",
        "  return Age\n",
        "\n",
        "\n",
        "func_udf = udf(func, IntegerType())\n",
        "df4 = df3.withColumn('new_column',func_udf(df3['Age'], df3['OREC']))\n",
        "\n",
        "drop_list = [ 'Age',]\n",
        "sdf5=df4.select([column for column in df4.columns if column not in drop_list])\n",
        "\n",
        "df_final = sdf5.withColumnRenamed(\"new_column\", \"Age\")\n",
        "\n",
        "df_final.show()\n"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+-------------------+------+----+--------+-------+----+-----+---+\n",
            "|Person ID|                DOB|Gender|OREC|LTIMCAID|NEMCAID|ESRD|MCAID|Age|\n",
            "+---------+-------------------+------+----+--------+-------+----+-----+---+\n",
            "| 54891178|1946-09-04 00:00:00|     2|   3|       1|      1|   0|    0| 74|\n",
            "|   227107|1952-07-29 00:00:00|     2|   2|       0|      0|   1|    1| 68|\n",
            "|  5155049|1950-08-11 00:00:00|     2|   0|       0|      0|   1|    1| 70|\n",
            "|  5766696|1948-05-06 00:00:00|     2|   3|       0|      0|   0|    0| 72|\n",
            "|   324986|1947-12-18 00:00:00|     2|   3|       0|      0|   1|    0| 72|\n",
            "|  4480355|1947-02-17 00:00:00|     1|   1|       0|      0|   0|    0| 73|\n",
            "|   779521|1949-04-28 00:00:00|     1|   1|       0|      0|   0|    0| 71|\n",
            "|   302256|1940-01-23 00:00:00|     2|   1|       0|      1|   0|    0| 80|\n",
            "|  8053184|1945-10-07 00:00:00|     1|   0|       0|      1|   0|    0| 75|\n",
            "| 55688818|1954-10-07 00:00:00|     1|   0|       0|      0|   0|    0| 66|\n",
            "| 62697035|1945-01-05 00:00:00|     2|   3|       1|      0|   0|    0| 75|\n",
            "|  7351480|1947-01-01 00:00:00|     1|   0|       0|      1|   0|    0| 73|\n",
            "| 31590945|1954-07-26 00:00:00|     1|   3|       1|      0|   0|    0| 66|\n",
            "|  7766518|1948-07-25 00:00:00|     1|   0|       0|      1|   1|    1| 72|\n",
            "|   489180|1940-01-21 00:00:00|     1|   3|       1|      0|   0|    0| 80|\n",
            "|   244502|1944-01-29 00:00:00|     2|   0|       0|      0|   0|    0| 76|\n",
            "|  6147861|1949-12-07 00:00:00|     2|   2|       0|      0|   0|    0| 71|\n",
            "| 63115039|1945-05-01 00:00:00|     2|   2|       0|      0|   1|    0| 75|\n",
            "|   701453|1950-04-09 00:00:00|     1|   2|       1|      0|   0|    0| 70|\n",
            "|   663772|1948-05-08 00:00:00|     2|   0|       1|      0|   1|    0| 72|\n",
            "+---------+-------------------+------+----+--------+-------+----+-----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X59hH-wN4Ao1"
      },
      "source": [
        "###Please Refer above df_final(dataframe for further calculation)-Himanshu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BDh7FUDk0AP"
      },
      "source": [
        "#@title Demography Variable\n",
        "\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import datediff, to_date, lit\n",
        "from pyspark.sql.functions import col, expr, when\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "df_age = df_final.toPandas()\n",
        "df_final_copy = df_age.copy()\n",
        "data = spark.createDataFrame(df_final_copy)\n",
        "\n",
        "demo = (spark.read.format(\"csv\").options(header=\"true\").load(\"/content/Demography Variable Calculations.csv\"))\n",
        "\n",
        "demo = demo.filter((demo['Version'] == Choose_a_model_version) & (demo['Year'] == Choose_a_model_year))\n",
        "data = data.withColumn(\"Year\", lit(str(Choose_a_model_year)))\n",
        "data = data.withColumn(\"Version\", lit(str(Choose_a_model_version)))\n",
        "\n",
        "for i in demo.collect():\n",
        "  data = data.withColumn(i[2], expr(i[3]))\n",
        "\n",
        "data.show()\n",
        "\n",
        "#data.toPandas().to_excel('v24'+ '.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwj6p6h2laIS"
      },
      "source": [
        "#@title CC_Mapping\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "from pyspark.sql.functions import length,col,trim\n",
        "from pyspark.sql.functions import UserDefinedFunction\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_diagnosis = pd.read_csv('Diagnosis Input File.csv')\n",
        "df_diagnosis = spark.createDataFrame(df_diagnosis)\n",
        "df_diagnosis = df_diagnosis.withColumn('DIAGNOSIS CODE',trim(col(\"DIAGNOSIS CODE\")))\n",
        "\n",
        "df_CC_mapping = pd.read_csv('CC Mapping.csv')\n",
        "label_schema = StructType([\n",
        "    StructField(\"YEAR\", StringType()),\n",
        "    StructField(\"VERSION\", StringType()),\n",
        "    StructField(\"DIAGNOSIS CODE\", StringType()),\n",
        "    StructField(\"CC\", IntegerType())\n",
        "])\n",
        "df_CC_mapping = spark.createDataFrame(df_CC_mapping,schema= label_schema)\n",
        "\n",
        "\n",
        "start=time.time()\n",
        "# for year in model_years:\n",
        "#   for version in versions:\n",
        "cond1= col('YEAR')== str(Choose_a_model_year)\n",
        "cond2= col('VERSION')== str(Choose_a_model_version)\n",
        "df_temp= df_CC_mapping.where(cond1 & cond2)\n",
        "\n",
        "# df_temp.show(5)\n",
        "df_temp= df_temp.toDF('YEAR','VERSION','D_DIAG CODE','CC')\n",
        "\n",
        "h= df_temp.join(df_diagnosis,(df_diagnosis[\"DIAGNOSIS CODE\"] == df_temp[\"D_DIAG CODE\"]),how='right')  # Mtachin diagnosis codes from diagnosis input file and CC_mapping file\n",
        "\n",
        "unique_HCC= sorted([i.CC for i in df_temp.select('CC').distinct().collect()]) #Finding all unique values of HCC and storing it in a list to generate HCC columns later\n",
        "\n",
        "CC_list = h.select(\"CC\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "p_id = h.select(\"PERSON ID\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "diag_code= h.select(\"DIAGNOSIS CODE\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "\n",
        "df_main= pd.DataFrame()\n",
        "df_main['PERSON ID']= p_id\n",
        "df_main['DIAGNOSIS CODE']= diag_code \n",
        "\n",
        "df_main['HCC']= CC_list  # CC values obtained after mapping from Part 2.\n",
        "df_main['HCC'] = list(df_main['HCC'].astype(np.float).astype(\"Int32\"))\n",
        "\n",
        "df_main['MODEL YEAR']= str(Choose_a_model_year)\n",
        "df_main['Version']=str(Choose_a_model_version)\n",
        "\n",
        "# df_main\n",
        "if 'RX' in Choose_a_model_version:\n",
        "  columns_df=[]\n",
        "  for value in (unique_HCC):\n",
        "    columns_df.append('RXHCC'+str(value)) # Example: RXHCC1,RXHCC2,RXHCC3 etc\n",
        "  #print(columns_df)\n",
        "  for colum in range(len(columns_df)):\n",
        "    df_main[columns_df[colum]]=0  #adding zero's to every column \n",
        "\n",
        "  for v in range(len(df_main)):                         \n",
        "    df_main.at[v,'RXHCC'+ str(df_main.iloc[v][2])]=1   # Adding 1's to columns. Example if df_main['HCC'][10]= 22, then adding a 1 in HCC10 column in the 9th row. \n",
        "\n",
        "  df_main = df_main.drop(df_main.columns[-1],axis=1)\n",
        "  #df_main.to_excel(str(Choose_a_model_version)+'_'+str(Choose_a_model_year)+'.xlsx')\n",
        "\n",
        "\n",
        "else:\n",
        "  # Part 5: Creating new columns with column names as HCC values obtained from Part 4\n",
        "  columns_df=[]\n",
        "  for value in (unique_HCC):\n",
        "    columns_df.append('HCC'+str(value)) # Example: HCC1,HCC2,HCC3 etc\n",
        "  #print(columns_df)\n",
        "  for colum in range(len(columns_df)):\n",
        "    df_main[columns_df[colum]]=0  #adding zero's to every column \n",
        "\n",
        "  for v in range(len(df_main)):                         \n",
        "    df_main.at[v,'HCC'+ str(df_main.iloc[v][2])]=1   # Adding 1's to columns. Example if df_main['HCC'][10]= 22, then adding a 1 in HCC10 column in the 9th row. \n",
        "\n",
        "  df_main = df_main.drop(df_main.columns[-1],axis=1)\n",
        "  #df_main.to_excel(str(Choose_a_model_version)+'_'+str(Choose_a_model_year)+'.xlsx')\n",
        "\n",
        "print(time.time()-start)\n",
        "df_main"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3WbXE4JpwKU"
      },
      "source": [
        "## @CC-Override"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_QqGHHxUDvt"
      },
      "source": [
        "#@title CC-Override\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLYzINsVUbJC"
      },
      "source": [
        "type(df_main)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmbpA2GnUPJs"
      },
      "source": [
        "type(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADPf-alAUhIO"
      },
      "source": [
        "data = data.toPandas()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVPkd5MLUp9P"
      },
      "source": [
        "type(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lE2f-gffU-5T"
      },
      "source": [
        "data.rename(columns = {'Person ID':'PERSON ID'}, inplace = True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZZbf3Omz1yI"
      },
      "source": [
        "df = pd.merge(df_main,data,on =\"PERSON ID\", how=\"inner\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ah5nU6wk4A7A"
      },
      "source": [
        "df=df.drop(columns=['Year','Version_y'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPnyKwie4lak"
      },
      "source": [
        "df.rename(columns={'Version_x':'Version'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWewr43S4uOQ"
      },
      "source": [
        "type(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeJEFVGS0AHB"
      },
      "source": [
        "df.rename(columns={'DIAGNOSIS CODE':'DIAG', 'GENDER':'SEX', 'Age':'AGEF'}, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2mAL7pQ0IEh"
      },
      "source": [
        "versions = df['Version'].unique().tolist()\n",
        "versions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yElaM_z5oUd"
      },
      "source": [
        "years = df['MODEL YEAR'].unique().tolist()\n",
        "years"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-b-YRAUC5w-9"
      },
      "source": [
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLYmnd4RI2Vs"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQLXxTDhIxkU"
      },
      "source": [
        "df.to_excel('Merged_df1.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wabvP0ZmJTmp"
      },
      "source": [
        "data1 = pd.read_excel('/content/Merged_df1.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfmKpRvS0IJM"
      },
      "source": [
        "df1 = data1.copy()\n",
        "data = spark.createDataFrame(df1)\n",
        "    \n",
        "overide_df = (spark.read.format(\"csv\").options(header=\"true\").load(\"/content/drive/MyDrive/Index/CC Override Rules.csv\"))\n",
        "overide_df = overide_df.withColumnRenamed(\"MODEL YEAR\",\"MODEL_YEAR\")\n",
        "overide_df = overide_df.filter((overide_df.VERSION.isin(versions))&(overide_df.MODEL_YEAR.isin(years)))\n",
        "for i in overide_df.collect():\n",
        "  data = data.withColumn('HCC', expr(i[3]))\n",
        "data.toPandas().to_excel('Merged_CCOveride.xlsx')  \n",
        "    \n",
        "#Merged file for CCOverride"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3prDvu00ILh"
      },
      "source": [
        "df_CCmerged = pd.read_excel('/content/Merged_CCOveride.xlsx')\n",
        "df_CCmerged"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEx1YIzT0IN9"
      },
      "source": [
        "df_CCmerged=df_CCmerged.drop(columns=['Unnamed: 0',\t'Unnamed: 0.1'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFMxsO8U0IRW"
      },
      "source": [
        "dRX=df_CCmerged[df['Version']== 'RX']\n",
        "dRX = dRX.reset_index(drop=True)\n",
        "dRX"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0In0CCH0aMj"
      },
      "source": [
        "Versions=df_CCmerged['Version'].unique()\n",
        "Versions\n",
        "years=df_CCmerged['MODEL YEAR'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMuJ39FvJ_oW"
      },
      "source": [
        "Versions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c_FqdvmY0aPW"
      },
      "source": [
        "result1=[]\n",
        "for version, df_version in df_CCmerged.groupby('Version'):\n",
        "  for year, df_year in df_version.groupby('MODEL YEAR'):\n",
        "    result1.append(df_year)\n",
        "\n",
        "result = []\n",
        "for dat in result1:\n",
        "  dat = result.append(dat.groupby(['PERSON ID'] ).max().reset_index())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVcg9-eY0aSG"
      },
      "source": [
        "[d1]=result\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_zjS_5KNXM"
      },
      "source": [
        "final = pd.concat(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f567CbALV-w"
      },
      "source": [
        "final.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4HUbDkxLh7X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO2h40FHHFEk"
      },
      "source": [
        "final.to_excel('Merged_rowdf.xlsx') #merged data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YucQisOZ0aVc"
      },
      "source": [
        "\n",
        "#Files for single row for single personID for each year and version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov-C6DF0PHFA"
      },
      "source": [
        "d1.to_excel('ESRD-P1-2020.xlsx')\n",
        "d2.to_excel('ESRD-P1-2021.xlsx')\n",
        "d3.to_excel('ESRD-P2-2020.xlsx')\n",
        "d4.to_excel('ESRD-P2-2021.xlsx')\n",
        "d5.to_excel('RX-2020.xlsx')\n",
        "d6.to_excel('RX-2021.xlsx')\n",
        "d7.to_excel('V22-2020.xlsx')\n",
        "d8.to_excel('V22-2021.xlsx')\n",
        "d9.to_excel('V24-2020.xlsx')\n",
        "d10.to_excel('V24-2021.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDt22u4n0kRu"
      },
      "source": [
        "-- END OF CC-OVERRIDE with single row for each person ID for for each year and version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Vfc1PKRMvp"
      },
      "source": [
        "HCC HIERARCHY STARTS HERE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDM54wsb0o__",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "0d40b7dc-bc7c-48e6-c1dd-b3135433ba2c"
      },
      "source": [
        "data = pd.read_excel('/content/gdrive/My Drive/HCC Hierarchy (1).xlsx')   #Loading Files\n",
        "data1 = spark.createDataFrame(data)                                       #Making Spark Dataframe\n",
        "data1.show()                                                              #Making Spark Dataframe\n",
        "merged_cc = pd.read_excel('/content/gdrive/My Drive/Merged_rowdf.xlsx')  #Loading File Created by HCC Hierarchy Team\n",
        "\n",
        "versions = data['MODEL_VRSN'].unique()                                    #Taking unique values to iterate later\n",
        "years = data['MODEL_YR'].unique()                                         #Taking unique values to iterate later\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "#Section to create hierarchy table for all unique years and version being passed from above. This ensure it to be dynamic.\n",
        "lis = {}\n",
        "for k in range(len(years)):\n",
        "  for l in range(len(versions)):\n",
        "    var = str(years[k]) + '_' +  str(versions[l])\n",
        "    lis[var] = []\n",
        "    data_2020_test = data[(data['MODEL_YR'] == years[k]) & (data['MODEL_VRSN'] == versions[l])]\n",
        "    merged_cc_2020_test = merged_cc[(merged_cc['MODEL YEAR'] == years[k]) & (merged_cc['Version'] == versions[l]) ]\n",
        "    merged_cc_2020_spark_test = spark.createDataFrame(merged_cc_2020_test)\n",
        "    for i in range(len(data_2020_test)):\n",
        "      merged_cc_2020_spark1 = 0\n",
        "      if list(data_2020_test['HIGHER_HCC'])[i] in merged_cc_2020_spark_test.columns:\n",
        "        merged_cc_2020_spark1 = merged_cc_2020_spark_test.withColumn(list(data_2020_test['LOWER_HCC'])[i], F.when((F.col(list(data_2020_test['HIGHER_HCC'])[i])==1) & (F.col(list(data_2020_test['LOWER_HCC'])[i])== 1) & (F.col('VERSION')== list(data_2020_test['MODEL_VRSN'])[i]) ,0).otherwise(F.col(list(data_2020_test['LOWER_HCC'])[i])))\n",
        "        # print(i)\n",
        "    lis[var].append(merged_cc_2020_spark1)\n",
        "\n",
        "\n",
        "\n",
        "# Creating seperate files for different years and versions and saving excel files.\n",
        "mer_frames = []\n",
        "for i in lis.keys():\n",
        "  print(i)\n",
        "  if lis[i][0]!=0:  \n",
        "    frame = lis[i][0]\n",
        "    frame = frame.toPandas()\n",
        "    mer_frames.append(frame)\n",
        "    frame.to_excel('/content/gdrive/My Drive/' + str(i) + '.xlsx')\n",
        "\n",
        "#Creating merged database to be passed to the interaction variables calculation.\n",
        "result = pd.concat(mer_frames)\n",
        "\n",
        "#Savin\n",
        "result.to_excel('/content/gdrive/My Drive/Merged_file_for_Akarshika_updated.xlsx')\n",
        "\n",
        "\n"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-265-08fbde26df46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/HCC Hierarchy (1).xlsx'\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#Loading Files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m                                       \u001b[0;31m#Making Spark Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                                              \u001b[0;31m#Making Spark Dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmerged_cc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/Merged_rowdf.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#Loading File Created by HCC Hierarchy Team\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/HCC Hierarchy (1).xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYfXxAYuYFw5"
      },
      "source": [
        "END OF HCC HIERARCHY\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ww_4SKdl9h7-"
      },
      "source": [
        "Interaction Variable Calculation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF6JxSlh9o0Y"
      },
      "source": [
        "from pyspark.sql.functions import datediff, to_date, lit\n",
        "from pyspark.sql.functions import col, expr, when\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "result= pd.read_csv('Merged_file_for_HCC_updated.csv')\n",
        "df1_copy = result.copy()\n",
        "data_interaction = spark.createDataFrame(df1_copy)\n",
        "\n",
        "interaction_demo = (spark.read.format(\"csv\").options(header=\"true\").load(\"/content/Interaction Variable Calculations - 1.csv\"))\n",
        "\n",
        "interaction_demo = interaction_demo.filter((interaction_demo['MODEL VRSN'] == Choose_a_model_version) & (interaction_demo['MODEL YR'] == Choose_a_model_year))\n",
        "data_interaction = data_interaction.where(F.col('MODEL YEAR') == Choose_a_model_year)\n",
        "data_interaction = data_interaction.withColumn(\"Version\", lit(str(Choose_a_model_version)))\n",
        "\n",
        "for i in interaction_demo.collect():\n",
        "  data_interaction = data_interaction.withColumn(i[2], expr(i[3]))\n",
        "\n",
        "data_interaction.toPandas().to_excel('Merged_Interaction_Variable.xlsx') \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSwQ5TUyHa4b"
      },
      "source": [
        "End of Interaction Variable Calculation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGmryPaFHgnn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}